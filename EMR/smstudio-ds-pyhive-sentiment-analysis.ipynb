{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Overview\n",
    "\n",
    "This notebook does the following:\n",
    "\n",
    "* Demonstrates how you can visually connect Amazon SageMaker Studio Python 3 (Data Science) kernel to a LDAP protected EMR Cluster or a cluster with no Authentication\n",
    "* Explore and query data from a Hive table using the pyhive library\n",
    "* Demonstrates how to use the data for Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connection to  EMR Cluster\n",
    "\n",
    "For the LDAP cluster, when prompted to enter username and password, use **\"david\"** for username and **\"welcome123\"** for password. \n",
    "\n",
    "In the cell below, the code block is autogenerated. You can generate this code by clicking on the \"Cluster\" link on the top of the notebook and select the EMR cluster. The \"j-xxxxxxxxxxxx\" is the cluster id of the cluster selected. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#LDAP \n",
    "\n",
    "# %load_ext sagemaker_studio_analytics_extension.magics\n",
    "# %sm_analytics emr connect --cluster-id j-xxxxxxxxxxxx --auth-type Basic_Access --language python"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NO AUTH\n",
    "# %load_ext sagemaker_studio_analytics_extension.magics\n",
    "# %sm_analytics emr connect --cluster_id j-xxxxxxxxxxx --auth_type None --language python"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we will import hive module from the pyhive library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyhive import hive"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, We use the private DNS name of the EMR primary in the following code. Replace the host with the correct DNS name. You can find this in the output of the CloudFormation stack under the key EMRMasterDNSName. You can also find this information on the Amazon EMR console (expand the cluster name and locate Master public DNS under in summary section)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conn = hive.Connection(host='<Enter EMRMasterDNSName>', port=10000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will query the movie_reviews table and get the data into a pandas dataframe. You can visualize the data using the code below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute('show databases')\n",
    "cursor.fetchall()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cursor.execute('show tables')\n",
    "cursor.fetchall()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "movie_reviews = pd.read_sql(\"select review, sentiment from movie_reviews\", conn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "movie_reviews.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos_reviews = movie_reviews.filter(movie_reviews.sentiment == 'positive')\n",
    "neg_reviews = movie_reviews.filter(movie_reviews.sentiment == 'negative')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_counts(positive,negative):\n",
    "    plt.rcParams['figure.figsize']=(6,6)\n",
    "    plt.bar(0,positive,width=0.6,label='Positive Reviews',color='Green')\n",
    "    plt.bar(2,negative,width=0.6,label='Negative Reviews',color='Red')\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Type of Review')\n",
    "    plt.tick_params(\n",
    "        axis='x',          \n",
    "        which='both',      \n",
    "        bottom=False,      \n",
    "        top=False,         \n",
    "        labelbottom=False) \n",
    "    plt.show()\n",
    "    \n",
    "plot_counts(len(pos_reviews),len(neg_reviews))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "next, we will use SageMaker experiments, trial and estimator to train a model and deploy the model using SageMaker realtime endpoint hosting\n",
    "\n",
    "In the next cell, we will install the necessary libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U  \"sagemaker>=1.72.1,<2.0.0\"  \n",
    "!{sys.executable} -m pip install sagemaker-experiments \n",
    "!{sys.executable} -m pip show sagemaker"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will import libraries and set global definitions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "from time import strftime, gmtime\n",
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sess = boto3.Session()\n",
    "region_name = sess.region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_runtime = boto3.Session().client('sagemaker-runtime')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next cell, we will create a new S3 bucket that will be used for storing the training and validation data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stsclient = boto3.client(\"sts\", region_name=region_name)\n",
    "s3client = boto3.client(\"s3\", region_name=region_name)\n",
    "\n",
    "aws_account_id = stsclient.get_caller_identity()[\"Account\"] \n",
    "bucket = \"sagemaker-studio-pyhive-{}-{}\".format(region_name, aws_account_id)\n",
    "key = \"sentiment/movie_reviews.csv\"\n",
    "smprocessing_input = \"s3://{}/{}\".format(bucket, key)\n",
    "\n",
    "try:\n",
    "    if region_name==\"us-east-1\":\n",
    "           s3client.create_bucket(Bucket=bucket)\n",
    "    else:\n",
    "           s3client.create_bucket(Bucket=bucket, CreateBucketConfiguration={\n",
    "                'LocationConstraint': region_name})\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    message = e.response['Error']['Message']\n",
    "    if error_code == 'BucketAlreadyOwnedByYou':\n",
    "        print ('A bucket with the same name already exists in your account - using the same bucket.')\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Error->{}:{}\".format(error_code, message))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upload the data to the S3 bucket"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import boto3\n",
    "from io import StringIO\n",
    "csv_buffer = StringIO()\n",
    "movie_reviews.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-process data and feature engineering\n",
    "\n",
    "#### Amazon SageMaker Processing jobs using the Scikit-learn Processor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-process data and feature engineering\n",
    "Amazon SageMaker Processing jobs using the Scikit-learn Processor\n",
    "With Amazon SageMaker Processing jobs, you can leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform.\n",
    "\n",
    "A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "The cell below shows how to run scikit-learn scripts using a Docker image provided and maintained by SageMaker to preprocess data.\n",
    "\n",
    "Note: We will use a \"ml.m5.xlarge\" instance as the instance type for sagemaker processing, training and model hosting. If you don't have access to this instance type and see a \"ResourceLimitExceeded\" error, use another instance type that you have access to. You can also request a service limit increase using AWS Support Center"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "instance_type_smprocessing=\"ml.m5.xlarge\"\n",
    "instance_type_smtraining=\"ml.m5.xlarge\"\n",
    "instance_type_smendpoint=\"ml.m5.xlarge\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type=instance_type_smprocessing,\n",
    "                                     instance_count=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(smprocessing_input)\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=smprocessing_input,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='validation_data',\n",
    "                                                source='/opt/ml/processing/validation')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'validation_data':\n",
    "        preprocessed_validation_data = output['S3Output']['S3Uri']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(preprocessed_training_data)\n",
    "print(preprocessed_validation_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prefix = 'blazingtext/supervised' \n",
    "s3_train_data = preprocessed_training_data\n",
    "s3_validation_data = preprocessed_validation_data\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train a SageMaker model\n",
    "#### Amazon SageMaker Experiments\n",
    "\n",
    "Amazon SageMaker Experiments allows us to keep track of model training; organize related models together; and log model configuration, parameters, and metrics to reproduce and iterate on previous models and compare models. \n",
    "Let's create the experiment, trial, and train the model. To reduce cost, the training code below uses spot instances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sm_session = sagemaker.session.Session()\n",
    "\n",
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "sentiment_experiment = Experiment.create(experiment_name=\"sentimentdetection-{}\".format(create_date), \n",
    "                                              description=\"Detect sentiment in text\", \n",
    "                                              sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "trial = Trial.create(trial_name=\"sentiment-trial-blazingtext-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())), \n",
    "                     experiment_name=sentiment_experiment.experiment_name,\n",
    "                     sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_use_spot_instances = True\n",
    "train_max_run=3600\n",
    "train_max_wait = 3600 if train_use_spot_instances else None\n",
    "\n",
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type=instance_type_smtraining,\n",
    "                                         train_volume_size = 30,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sm_session,\n",
    "                                         train_use_spot_instances=train_use_spot_instances,\n",
    "                                         train_max_run=train_max_run,\n",
    "                                         train_max_wait=train_max_wait)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.005328,\n",
    "                            vector_dim=286,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "bt_model.fit(data_channels, \n",
    "             experiment_config={\n",
    "                      \"ExperimentName\": sentiment_experiment.experiment_name, \n",
    "                      \"TrialName\": trial.trial_name,\n",
    "                      \"TrialComponentDisplayName\": \"BlazingText-Training\",\n",
    "                  },\n",
    "             logs=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deploy the model and get predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1, instance_type = instance_type_smendpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "review = [\"please give this one a miss br br kristy swanson and the rest of the cast\"\n",
    "          \"rendered terrible performances the show is flat flat flat br br\"\n",
    "          \"i don't know how michael madison could have allowed this one on his plate\"\n",
    "          \"he almost seemed to know this wasn't going to work out\"\n",
    "          \"and his performance was quite lacklustre so all you madison fans give this a miss\"]\n",
    "tokenized_review = [' '.join(t.split(\" \")) for t in review]\n",
    "#For retrieving the top k predictions, you can set k in the configuration\n",
    "payload = {\"instances\" : tokenized_review}\n",
    "bt_endpoint_name = text_classifier.endpoint\n",
    "response = sm_runtime.invoke_endpoint(EndpointName=bt_endpoint_name,\n",
    "                                      ContentType = 'application/json',\n",
    "                                      Body=json.dumps(payload))\n",
    "output = json.loads(response['Body'].read().decode('utf-8'))\n",
    "#make the output readable \n",
    "import copy\n",
    "predictions = copy.deepcopy(output) \n",
    "for output in predictions:\n",
    "    output['label'] = output['label'][0][9:].upper() \n",
    "print(predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean up  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Clean up resources created as part of this notebook\n",
    "#delete endpoint\n",
    "bt_model.delete_endpoint()\n",
    "# empty s3 bucket we created\n",
    "s3_bucket_to_remove = \"s3://{}\".format(bucket)\n",
    "!aws s3 rm {s3_bucket_to_remove} --recursive"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}